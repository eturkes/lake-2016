---
title: "Lake 2016 snRNAseq BA10 Report"
author:
  - name: "Emir Turkes"
  - name: "Columbia University"
date: '`r strftime(Sys.time(), format = "%B %d, %Y")`'
bibliography: "../lake-2016-snRNAseq.bib"
biblio-style: apalike
link-citations: true
output:
  html_document:
    number_sections: true
    theme: lumen
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
knit:
  (function(inputFile, encoding) {
    rmarkdown::render(
      inputFile, encoding = encoding, output_file = "../results/lake-2016-snRNAseq-BA10-report.html")})
---

```{r, include = FALSE}
#    This file is part of lake-2016-snRNAseq.
#    Copyright (C) 2019  Emir Turkes
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
#    Emir Turkes can be contacted at emir.turkes@eturkes.com

knitr::opts_chunk$set(fig.width = 8.5)
```

<style type="text/css">
body {font-size: 16px;}
h1.title {font-size: 35px;}
h1 {font-size: 24px;}
h2 {font-size: 22px;}
h3 {font-size: 20px;}
.toc-content {padding-left: 0px; padding-right: 0px;}
div.tocify {width: 100%;}
.tocify-subheader .tocify-item {font-size: 0.95em; padding-left: 25px; text-indent: 0;}
div.main-container {max-width: none; width: 100%;}
</style>

*This is a broad initial analysis that prepares and characterizes the data for use in other projects.*

The background for this data is as follows:

- dbGaP Accession: [phs000833](https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id=phs000833.v7.p1).
- Part of [SCAP-T](https://www.scap-t.org/) and originally published in @lake_neuronal_2016.
- Single patient, age = 51, sex = female, NeuN + nuclei from normal mostmortem.
- 6 cortical regions: BA8, BA10, BA21, BA22, BA41, and BA17.
- Annotated into 8 excitatory and inhibitory neuronal subtypes.

This analysis was performed in R except where noted.
The source code and instructions for rerunning the analysis can be found at [github.com/eturkes/lake-2016-snRNAseq](https://github.com/eturkes/lake-2016-snRNAseq).

# Final Results

**Read just this section for the final results of the analysis and a summary of the methods.**

# ~~~ Breakdown of Methods ~~~ {-}

The following top-level sections break down the methods used to perform the analysis and only needs to be read if one is interested.
We start by setting some global variables and loading in any required packages.

```{r}
data_dir <- file.path(getwd(), "..", "data")
assets_dir <- file.path(getwd(), "..", "assets")
results_dir <- file.path(getwd(), "..", "results")
```

```{r}
library(conflicted)
library(devtools)
library(DT)
library(SingleCellExperiment)
library(scater)
library(ggplot2)
library(scran)
```

# Original Data {.tabset}

This section provides a brief look at the raw data before manipulation.

## Counts

```{r, cache = TRUE}
counts <- read.delim(
  file.path(data_dir, "Lake-2016_Gene_TPM.dat.gz"), "\t", header = FALSE, stringsAsFactors = FALSE)
```

```{r}
datatable(counts[1:5, 1:3])
```

## Clusters

```{r}
clust <- read.table(file.path(data_dir, "Lake-2016_Gene_TPM_Sample-annotation.txt"), header = TRUE)
datatable(clust[1:5, ])
```

# Preliminary Cleaning

Here we provide more informative labels to things, subset to the regions/cell types we are interested in, and transform the data into more convenient formats for downstream analysis.

## Relabeling and Subsetting {.tabset}

A more elegant solution is planned, but for now, the desired regions/cell types can be set here.

```{r}
# TODO: Simplify formatting requirements for the variables.
regions <- "BA10"
cell_types <- "Ex1|Ex2|Ex3|Ex4|Ex5|Ex6|Ex7|Ex8|In1|In2|In3|In4|In5|In6|In7|In8"
```

### Counts

```{r}
gene_names <- counts[ , 1]
cell_names <- counts[1, ]
cell_names <- as.character(unlist(cell_names))
cell_names <- cell_names[-1]
gene_names <- gene_names[-1]
counts <- counts[-1, -1]
exclude <- duplicated(gene_names)
keep_cells <- cell_names %in% clust[ , 2]
counts <- counts[ , keep_cells]
cell_names <- cell_names[keep_cells]
colnames(counts) <- cell_names
reorder <- order(colnames(counts))
counts <- counts[ , reorder]

clust <- clust[clust[ , 2] %in% colnames(counts), ]
clust <- clust[order(clust[ , 2]), ]
keep_source <- as.character(unlist(clust[ , 4])) %in% regions
counts <- counts[ , keep_source]

datatable(counts[1:5, 1:3])
```

### Clusters

```{r}
clust <- clust[clust[ , 2] %in% colnames(counts), ]
clust <- clust[order(clust[ , 2]), ]

source <- as.character(unlist(clust[ , 4]))
batch <- as.character(unlist(clust[ , 4]))
cell_type <- as.character(unlist(clust[3]))
age <- rep("51yo", times = length(batch))
sample <- colnames(counts)
sample <- paste0("X", sample)
data <- apply(counts, 1, as.numeric)
data <- t(data)
colnames(data) <- colnames(counts)
data <- data[!exclude, ]
rownames(data) <- gene_names[!exclude]
clust <- data.frame(sample = sample, cell_type = cell_type, source = source)
rownames(clust) <- colnames(counts)

datatable(clust[1:5, ])
```

## SingleCellExperiment

```{r}
sce <- SingleCellExperiment(
  assays = list(counts = as.matrix(sapply(counts, as.numeric))), colData = clust,
  rowData = gene_names)
sce
```

# QC

These sections assess and correct issues and irregularities in the dataset.

## Normalization

Use the the log2 transformation of the transcript counts.

```{r}
logcounts(sce) <- log2(counts(sce) + 1)
rowData(sce)$feature_symbol <- rowData(sce)
```

## Duplicates

Remove features with duplicated names.

```{r}
sce <- sce[!duplicated(rowData(sce)$feature_symbol), ]
```

## ERCC Spike-ins

Identify ERCC spike-ins.

```{r}
isSpike(sce, "ERCC") <- grepl("^ERCC-", rownames(sce))
```

## Library Size

Plot the library size.

```{r}
cell_types <- sort(colnames(colData(sce))[grepl("cell_type", colnames(colData(sce)))])
# for (ct in cell_types) {
#   print(plotColData(sce, aes_string(x = "total_counts", y = "total_features", colour = ct)))}
```

## Detected Genes

Plot the detected genes.

```{r}
# for (ct in cell_types) {
#   print(plotColData(
#     sce, aes_string(x = ct, y = "total_counts", colour = "log10_total_counts")) +
#       theme(axis.text.x = element_text(angle = 90, hjust = 1)))}
```

## Expression

Plot metrics about the expression of features.

```{r}
# plotHighestExprs(sce)
```

```{r}
# plotExprsFreqVsMean(sce)
```

## Mito/Ribo Genes

Using an annotation from [HGNC](https://www.genenames.org/), we first assess the mitochondrial/ribosomal gene population using histograms and scatter plots.

```{r, cache = TRUE}
ribo_genes <- read.table(
  file.path(assets_dir, "ribo-genes.txt"), sep = "\t",
  header = TRUE, stringsAsFactors = FALSE)
is_mito <- which(rowData(sce)$chromosome_name == "MT")
is_ribo <- which(rowData(sce)$external_gene_name %in% ribo_genes$Approved.Symbol)
sce <- calculateQCMetrics(sce, feature_controls = list(Mt = is_mito, Ri = is_ribo))
```

```{r}
par(mfrow = c(2, 2), mar = c(5, 4, 1, 1), bty = "n")
hist(
  log10(sce$total_counts), xlab = "log10(Library Sizes)", main = "",
  breaks = 20, col = "grey80", ylab = "Number of Cells")
hist(
  log10(sce$total_features_by_counts), xlab = "log10(Number of Expressed Genes)", main = "",
  breaks = 20, col = "grey80", ylab = "Number of Cells")
hist(
  sce$pct_counts_Ri, xlab = "Ribosomal Proportion Percentage", ylab = "Number of Cells",
  breaks = 40, main = "", col = "grey80")
hist(
  sce$pct_counts_Mt, xlab = "Mitochondrial Proportion Percentage", ylab = "Number of Cells",
  breaks = 80, main = "", col = "grey80")
par(mfrow = c(2, 2), mar = c(5, 4, 1, 1), bty = "n")
smoothScatter(
  log10(sce$total_counts), log10(sce$total_features_by_counts),
  xlab = "log10(Library Sizes)", ylab = "log10(Number of Expressed Genes)")
smoothScatter(
  log10(sce$total_counts), sce$pct_counts_Ri,
  xlab = "log10(Library Sizes)", ylab = "Ribosomal Proportion Percentage")
smoothScatter(
  log10(sce$total_counts), sce$pct_counts_Mt,
  xlab = "log10(Library Sizes)", ylab = "Mitochondrial Proportion Percentage")
smoothScatter(
  sce$pct_counts_Ri,sce$pct_counts_Mt, xlab = "Ribosomal Proportion Percentage",
  ylab = "Mitochondrial Proportion Percentage")
```

We then use the `isOutlier` function from `scater` to remove the the mitochondrial/ribosomal genes.

```{r}
libsize_drop <- isOutlier(sce$total_counts, nmads = 3, type = "lower", log = TRUE)
feature_drop <- isOutlier(sce$total_features_by_counts, nmads = 3, type = "lower", log = TRUE)
mito_drop <- isOutlier(sce$pct_counts_Mt, nmads = 3, type = "higher")
ribo_drop <- isOutlier(sce$pct_counts_Ri, nmads = 3, type = "higher")
keep <- !(libsize_drop | feature_drop | mito_drop | ribo_drop)
sce <- sce[ , keep]
```

## Normalization

We use `computeSumFactors` from the `scran` package to perform normalization.
This function uses a linear deconvolution system to account for expected variation across different cell types/sizes [@l_lun_pooling_2016].
We also remove cells that have size factors from the function that are very small ($< 0.01$) or negative.
To verify the validity of this method, we first plot each cell's size factor against their total counts.

```{r, cache = TRUE}
clusters <- quickCluster(sce, min.mean = 0.1, method = "igraph")
sce <- computeSumFactors(sce, cluster = clusters, min.mean = 0.1)
```

```{r}
sce <- sce[ , which(sizeFactors(sce) > 0.01)]
par(mfrow = c(1, 2), mar = c(5, 4, 2, 1), bty = "n")
smoothScatter(
  sce$total_counts, sizeFactors(sce), log = "xy", xlab = "Total Counts", ylab = "Size Factors")
plot(
  sce$total_counts, sizeFactors(sce), log = "xy", xlab = "Total Counts",
  ylab = "Size Factors", cex = 0.3, pch = 20, col = rgb(0.1, 0.2, 0.7, 0.3))
abline(h = 0.05)
```

As the two plots appear to be highly correlated, we go ahead and perform the normalization.

```{r}
sce <- normalize(sce)
```

# Dimensionality Reduction

This section details steps taken to reduce the dimensionality of the dataset through methods like PCA, tSNE, UMAP, etc.

## HVG

We start by identifying highly variable genes (HVGs), which is the first step in many PCA/tSNE approaches.
HVGs are those that exhibit a high amount of biological signal relative to background noise.
First, let's visualize variability in our dataset.

```{r, cache = TRUE}
new_trend <- makeTechTrend(x = sce)
```

```{r}
fit <- trendVar(sce, use.spikes = FALSE, loess.args = list(span = 0.05))
par(mfrow = c(1, 1), mar = c(5, 4, 2, 1), bty = "n")
plot(
  fit$mean, fit$var, pch = 20,
  col = rgb(0.1, 0.2, 0.7, 0.6), xlab = "log(Mean)", ylab = "Variance")
curve(fit$trend(x), col = "orange", lwd = 2, add = TRUE)
curve(new_trend(x), col = "red", lwd = 2, add = TRUE)
legend(
  "top", legend = c("Poisson Noise", "Observed Trend"), lty = 1,
  lwd = 2, col = c("red", "orange"), bty = "n")
```

And then plot the HVGs themselves.

```{r}
fit$trend <- new_trend
dec <- decomposeVar(fit = fit)
top_dec <- dec[order(dec$bio, decreasing = TRUE), ]
plotExpression(sce, features = rownames(top_dec)[1:10])
```

## PCA

We start by performing PCA on all of the genes rather than just HVGs using denoisePCA from the scran package. This function automatically selects PCs by modelling technical noise.

```{r, cache = TRUE}
sce <- denoisePCA(sce, technical = new_trend, approx = TRUE)
```

We plot principal components using `plotPCA` from the `scater` package.

```{r}
df_redDim <- data.frame(
  colData(sce)[ , c("sample", "cell_type", "source")], reducedDim(sce, "PCA")[ , 1:2],
  stringsAsFactors = FALSE)
rownames(df_redDim) <- NULL
par(mfrow = c(1, 1))
plot(
  log10(attr(reducedDim(sce), "percentVar")), xlab = "PC",
  ylab = "log10(Proportion of Variance Explained)", pch = 20,
  cex = 0.6, col = rgb(0.8, 0.2, 0.2, 0.5))
abline(v = ncol(reducedDim(sce, "PCA")), lty = 2, col = "red")

for (ct in cell_types) {
  print(plotPCA(sce, colour_by = ct))}
```

## tSNE {.tabset}

We use several approaches to identify clusters using tSNE, so this section is broken down into multiple subsections, with each tab referring to a particular set of coordinates.
Let's first define a function for consistant plotting throughout.

```{r}
ggplot_custom = function(data, x, y, col, type) {
	gg <- ggplot(data, aes_string(x = x, y = y)) +
		geom_point(size = 0.2, alpha = 0.6, aes_string(col = col)) +
		theme_classic() + theme(legend.position = "bottom")
	if (type == "cont") {
		gg <- gg + scale_colour_gradient(low = "blue", high = "red")
	} else if (type == "cat") {
		gg <- gg + guides(color = guide_legend(override.aes = list(size = 3)))
	}
	gg}
```

### All Genes

```{r, cache = TRUE}
sce <- runTSNE(sce, use_dimred = "PCA", perplexity = 30, rand_seed = 100)
```

```{r}
tmp_df <- data.frame(reducedDim(sce, "TSNE"), stringsAsFactors = FALSE)
rownames(tmp_df) <- NULL
names(tmp_df) <- paste0("new_tsne", 1:2)
df_redDim <- data.frame(df_redDim, tmp_df, stringsAsFactors = FALSE)
rm(tmp_df)
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "sample", type = "cat")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2",
  col = "log10_total_features_by_counts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "cell_type", type = "cat")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "source", type = "cat")
```

### HVG

Another way to cluster is by using the HVGs calculated earlier.
First, we select the top 1,000 HVGs using their FDR and biological residual thresholds and visualize them below.

```{r}
dec1 <- dec
dec1$bio[which(dec$bio < 1e-5)] <- 1e-5
dec1$FDR[which(dec$FDR < 1e-100)] <- 1e-100
par(mfrow = c(1, 2))
hist(log10(dec1$bio), breaks = 100, main = "")
hist(log10(dec1$FDR), breaks = 100, main = "")
w2kp <- which(dec$FDR < 1e-10 & dec$bio > 0.02)
sce_hvg <- sce[w2kp, ]
edat <- t(as.matrix(logcounts(sce_hvg)))
edat <- scale(edat)
```

Next, we calculate PCs using the log-transformed normalized expression data.

```{r, cache = TRUE}
ppk <- propack.svd(edat, neig = 50)
pca <- t(ppk$d*t(ppk$u))
tmp_df <- data.frame(pca[ , 1:2], stringsAsFactors = FALSE)
names(tmp_df) <- paste0("hvg_pc", seq(ncol(tmp_df)))
df_hvg <- data.frame(
  colData(sce)[ , c("sample", "cell_type", "source")], tmp_df, stringsAsFactors = FALSE)
rownames(df_hvg) <- NULL
```

```{r, cache = TRUE}
set.seed(100)
tsne <- Rtsne(pca, pca = FALSE)
```

```{r}
tmp_df <- data.frame(tsne$Y, stringsAsFactors = FALSE)
names(tmp_df) <- paste0("hvg_tsne", seq(ncol(tmp_df)))
df_hvg <- data.frame(df_hvg, tmp_df, stringsAsFactors = FALSE)
```

And finally, calculate the t-SNE plots.

```{r}
reducedDims(sce_hvg) <- SimpleList(PCA = pca, TSNE = tsne$Y)
ggplot_custom(
  data = df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "cell_id_stem", type = "cat")
ggplot_custom(
  data = df_hvg, x = "hvg_tsne1", y = "hvg_tsne2",
  col = "log10_total_features_by_counts", type = "cont")
ggplot_custom(
  data = df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "habib_cluster", type = "cat")
ggplot_custom(
  data = df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "habib_cluster_name", type = "cat")
ggplot_custom(
  data = df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "habib_cell_type", type = "cat")
```

# Clustering

The above section simply altered the dimensionality or visualization of the previously established clusters in @lake_neuronal_2016.
Now, we use various methods to generate new clusters.

## k-means

We apply a k-means method atop the 50 PCs generated earlier, choosing to generate 2-16 clusters.

```{r, cache = TRUE}
all_num_clust <- c(2:16)
df_redDim <- df_redDim[ , !grepl("^KM_", names(df_redDim))]
for (num_clust in all_num_clust) {
  cat(paste0("k-means with ", num_clust, " clusters.\n"))
  kmeans_out <- kmeans(
    reducedDim(sce, "PCA"), centers = num_clust, iter.max = 1e8,
    nstart = 2500, algorithm = "MacQueen")
  km_label <- paste0("km_", num_clust, "_clusters")
  df_redDim[[km_label]] = as.factor(kmeans_out$cluster)
}
```

## SC3

As an alternative method, we generate another set of 2-16 clusters using SC3 [@kiselev_sc3_2017].

```{r, cache = TRUE}
rowData(sce)$feature_symbol <- rowData(sce)$external_gene_name
all_ks <- c(2:16)

# Hybrid SVM approach
# sce <- sc3(
#   sce, gene_filter = FALSE, ks = all_ks, biology = TRUE, rand_seed = 100, svm_num_cells = 500)
# sce <- sc3_run_svm(sce, ks = all_ks)

sce <- sc3(sce, ks = all_ks, biology = TRUE)
```

```{r}
for (one_ks in all_ks) {
  sc3_label <- paste0("sc3_", one_ks, "_clusters")
  df_redDim[[sc3_label]] <- as.factor(colData(sce)[, sc3_label])}
p_data <- c("cell_type", "sample") # Metadata that we want included in SC3 figures.
```

# References

This is the concluding section of the document.
Here we write relevant results to disk, output the `session_info`, and create a bibliography for works cited.

```{r}
saveRDS(sce, file = file.path(results_dir, "data", "sce.rds"))
```

```{r}
session_info()
```
